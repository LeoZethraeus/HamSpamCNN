{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoZethraeus/HamSpamCNN/blob/master/assignment2notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h6HfhuXqFVsk"
      },
      "source": [
        "# Spam or Ham classification\n",
        "## Before we begin, a few words on my approach:\n",
        "I choose to implement my own classifier using a convolutional neural network with Keras. \n",
        "This is probably not the most conventional approach which might use something like a NaiveBayes algorithm. I do this because I saw in the job listing that you wanted to improve existing models using techniques from Computer Vision, so I wanted to give it a shot immediately. The resulting model yields 97% accuracy (on a separate test-set, not trained on) using only the text, no other features. Towards the end of this notebook I include an unpolished model using an ensemble prediction (using all included features), but I have not done any tuning of hyper-parameters, all the values (number of filters in CNN, nodes in Dense layers, the weights of the different models etc) are just randomly guessed, so I believe it can be improved. With the current values, the model taking only text as input performs better than the ensemble prediction.\n",
        "\n",
        "As a bonus, I tested it on some sample posts not included in the data set (written by myself or found on FB) and it works quite well, as long as the post is in English and is related to hiring/subletting an apartment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CRwITY9JFVsm"
      },
      "source": [
        "## Prerequisities\n",
        "\n",
        "I'm going to use Tensorflow/Keras, Numpy and Pandas with Python 3.7.\n",
        "\n",
        "to install Tensorflow and Keras I recommend using Anaconda (download correct version Windows/Linux/Mac on: https://www.anaconda.com/distribution/ and follow instructions in the installer)\n",
        "\n",
        "Before you install tensorflow and Keras I recommend to create a virtual environment, either using conda or pip.\n",
        "\n",
        "Write the following lines in your terminal:\n",
        "\n",
        "**conda create -n assignment2 python=3.7 anaconda**\n",
        "\n",
        "**conda activate assignment2**\n",
        "\n",
        "**conda install tensorflow**\n",
        "\n",
        "**conda install keras**\n",
        "\n",
        "**conda install numpy**\n",
        "\n",
        "**conda install pandas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T-8bIm3wFVsn"
      },
      "source": [
        "## Reading dataset and some preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1GzxrUtpFVso",
        "outputId": "3843d904-c728-4ada-c731-2d46c8b73b2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "url=\"PATH_TO_DATASET\"\n",
        "dataframe = pd.read_csv(url)\n",
        "\n",
        "text, has_link, has_image, label = dataframe.values.T\n",
        "\n",
        "def pre_processing():\n",
        "    \"\"\"Prepare for one-hot-encoding\"\"\"\n",
        "    label[label == 'ham'] = 0\n",
        "    label[label == 'spam'] = 1  \n",
        "    \n",
        "    has_link[has_link == True] = 1\n",
        "    has_link[has_link == False] = 0\n",
        "    \n",
        "    has_image[has_image == True] = 1\n",
        "    has_image[has_image == False] = 0\n",
        "    \n",
        "pre_processing()\n",
        "\n",
        "\"\"\"One-hot encode labels and binary variables using Keras\"\"\"\n",
        "one_hot_labels = to_categorical(label)\n",
        "one_hot_has_link = to_categorical(has_link)\n",
        "one_hot_has_image = to_categorical(has_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VibAua-oFVst"
      },
      "source": [
        "After loading the data it's time to tokenize the text and present it as sequences of integers.\n",
        "For that I use Keras Tokenizer, which fits on the text with respect to the vocabulary of all posts included,\n",
        "to create a meaningful representation as sequences of integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zzNIlJMjFVsu",
        "outputId": "964cd5c5-8fd0-4d84-b77d-cb903f06a257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: \n",
            "[wanted] I'm looking for a roommate who want to share a room if we find a room�� The location I want is nearby rosslyn station! My budget will be under 700! And I want to move before at least 1st december! If you're interested, let me know :)\n",
            "\n",
            "Sequence: \n",
            "[58, 46, 18, 6, 2, 124, 97, 138, 4, 135, 2, 13, 17, 33, 158, 2, 18269, 5, 153, 12, 138, 8, 795, 2840, 198, 23, 186, 42, 28, 521, 443, 1, 12, 138, 4, 87, 444, 35, 842, 156, 451, 17, 250, 66, 157, 15, 104]\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\"\"\"I use Keras Tokenizer to fit the vocabulary of the dataset and create sequences of integers\n",
        "as representations of the tokens\"\"\"\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(text)\n",
        "\n",
        "sequences = t.texts_to_sequences(text)\n",
        "\n",
        "\"\"\"Print example sequence\"\"\"\n",
        "print(\"Text: \")\n",
        "print(text[1] +'\\n')\n",
        "print('Sequence: ')\n",
        "print(sequences[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vEnwc5WCFVsx"
      },
      "source": [
        "Next I need to find the maximum number of tokens in a posts, and zero-pad the rest because Keras only accepts fixed length-inputs.\n",
        "\n",
        "**N.B.** I noticed that one post is much longer than all others (number 7178, about 4 times longer than the second longest post), affecting the max_length greatly and thus model performance. One option would be to remove it and gain a reduction of weights to save training time, but here I keep it. After all, such spam might occur in real life, and the training time is acceptable anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4fAltNYNFVsy",
        "outputId": "bb66eaf8-919b-47af-ca31-11ae0c7f0283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of posts: 8611 \n",
            "\n",
            "Padded sequence:\n",
            "[58 46 18 ...  0  0  0]\n",
            "\n",
            "Original length of example sequence: \n",
            "47\n",
            "Length of zero-padded version:\n",
            "8611\n",
            "\n",
            "Vocabulary needed to include all words in the data set:\n",
            "38774\n",
            "\n",
            "Setting vocab_size to 40000.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Find the maximum length of words in a post and zero-pad all other posts\n",
        "   (because Keras need fixed-length input)\"\"\"\n",
        "\n",
        "max_length = np.max([len(tx) for tx in sequences])\n",
        "print('Max length of posts: %s \\n' % max_length)\n",
        "padded_posts = pad_sequences(sequences, maxlen = max_length, padding = 'post')\n",
        "\n",
        "\"\"\"This is included just for clarity, you get the idea..\"\"\"\n",
        "print('Padded sequence:')\n",
        "print(padded_posts[1])\n",
        "print('\\nOriginal length of example sequence: ')\n",
        "print(len(sequences[1]))\n",
        "print('Length of zero-padded version:')\n",
        "print(len(padded_posts[1]))\n",
        "\n",
        "\"\"\"I also need to find the vocabulary\"\"\"\n",
        "\"\"\"Find the vocabulary of all texts in the dataset\"\"\"\n",
        "print('\\nVocabulary needed to include all words in the data set:')\n",
        "print(len(set(t.word_counts)))\n",
        "vocab_size = 40000 # From line above, but increased to allow for a slightly more flexible model.\n",
        "print('')\n",
        "print('Setting vocab_size to %s.' % vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fGCGKfeWFVs1"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1gRBukKEFVs2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Import some useful Keras tools for a training a Neural Network\"\"\"\n",
        "from keras.layers import Input, Dense\n",
        "from keras.layers import Dropout, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import objectives\n",
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yJMwO-dSFVs4"
      },
      "source": [
        "In the cell below I build the model. The architecture is a bit arbitrary, but the important layers are the Embedding and the Conv1D. The number of filters and kernel_size in Conv1D can be changed.\n",
        "\n",
        "The Embedding layer creates a dense vector representation of the sequences of integers representing the posts. The embedding is learnt during training of the model. This is very useful because then the model is specialised for our purposes (the vocabulary that might be present in a typical ham or spam post in the Apalca FB group).\n",
        "\n",
        "To do that, we need to know the size of the vocabulary, and the max_length of the post, as previously computed.\n",
        "\n",
        "The output_dim is the dimension of the output vector space and can be considered another hyper-parameter to be optimized.\n",
        "After some experimentation I found 50 to be a good value.\n",
        "\n",
        "The filters in Conv1D are analogous to Features when doing feature selection, except in Deep Learning you don't need to decide which features to look for, only how many. Also a hyper-parameter. The kernel-size = 5 means that it sweeps through 5 words at a time. Can be tuned as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wVS_eo95FVs5",
        "outputId": "5be86fc8-9b4c-4e08-e5f4-21b3d4a39d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 8611)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 8611, 50)          2000000   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 8607, 50)          12550     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 1721, 50)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 86050)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                860510    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 2,873,082\n",
            "Trainable params: 2,873,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "'''Build the model and print out model summary'''\n",
        "input_text = Input(shape=(max_length,))\n",
        "emb  = Embedding(input_dim = vocab_size, input_length = max_length, output_dim = 50)(input_text)\n",
        "dense_text = Conv1D(50,(5,), activation = 'relu', strides = 1)(emb)#emb)\n",
        "dense_text2 = MaxPooling1D(5)(dense_text)\n",
        "flat = Flatten()(dense_text2)\n",
        "dense_text3 = Dense(10, activation = 'relu')(flat)#(dense_text2)\n",
        "drop = Dropout(0.5)(dense_text3)\n",
        "output_1 = Dense(2, activation = 'sigmoid')(drop)\n",
        "\n",
        "textmodel = Model(input_text, output_1)\n",
        "print(textmodel.summary())\n",
        "'''I like to use the adam optimizer for general purpose DL, because it works well in many situations. \n",
        "The loss is binary cross-entropy, suitable for binary labels'''\n",
        "textmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oUPMqDK9FVs8"
      },
      "source": [
        "Almost 3 million trainable parameters. If you want to reduce the number of trainable parameters I would remove post 7178 and thus reduce the vocabulary and max_length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vnjWMdfJFVs9"
      },
      "source": [
        "Before I train the model, I shuffle the data, and check that it's approximately stratified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XarfhplJFVs9",
        "outputId": "67bf2ce3-35ae-457c-8c27-ea783911b0ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set: 8000 posts, test_set 2000 posts\n",
            "Train/test ratio = 0.8 \n",
            "\n",
            "Stratification check:\n",
            "Number of spam posts in training set: 3993 of total: 8000\n",
            "Number of spam posts in test set: 1007 of total: 2000\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Don't trust a model trained on an unshuffled dataset, would you play poker with an unshuffled deck?\"\"\"\n",
        "length_of_dataset = len(text)\n",
        "\n",
        "def shuffle_indices(length_of_dataset):\n",
        "    ints = range(length_of_dataset)\n",
        "    indices = np.array(ints)\n",
        "    np.random.shuffle(indices)\n",
        "    return indices\n",
        "\n",
        "shuffled_indices = shuffle_indices(length_of_dataset)\n",
        "\n",
        "ratio = 0.2 #Change if you want to.\n",
        "split = length_of_dataset-int(length_of_dataset*ratio) # Train/test split ratio.\n",
        "\n",
        "x_train = padded_posts[shuffled_indices[:split]]\n",
        "x_test = padded_posts[shuffled_indices[split:]]\n",
        "\n",
        "y_data = one_hot_labels\n",
        "y_train = y_data[shuffled_indices[:split]]\n",
        "y_test = y_data[shuffled_indices[split:]]\n",
        "\n",
        "print('Train set: {} posts, test_set {} posts'.format(len(x_train),len(x_test)))\n",
        "\n",
        "print('Train/test ratio = {:.1f} \\n'.format(split/length_of_dataset))\n",
        "\n",
        "\"\"\"Check if train/test-split is approximately stratified:\"\"\"\n",
        "shuffled_labels = label[shuffled_indices]\n",
        "print('Stratification check:')\n",
        "print('Number of spam posts in training set: {} of total: {}'.format(np.count_nonzero(shuffled_labels[:split]),split))\n",
        "print('Number of spam posts in test set: {} of total: {}'.format(np.count_nonzero(shuffled_labels[split:]), length_of_dataset-split))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZFnMpTtUFVtA"
      },
      "source": [
        "## Time to train the model. \n",
        "It takes a few minutes. (10 minutes on my CPU, 1 minute on my GPU but then you need to have the right CUDA drivers installed)\n",
        "\n",
        "**Run the cell below and refill your cup of coffee.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2HGNUaCeFVtB",
        "outputId": "e8f11e48-a903-4021-f514-354a48a88077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/4\n",
            "8000/8000 [==============================] - 147s 18ms/step - loss: 0.3955 - acc: 0.7928 - val_loss: 0.1094 - val_acc: 0.9720\n",
            "Epoch 2/4\n",
            "8000/8000 [==============================] - 145s 18ms/step - loss: 0.1518 - acc: 0.9558 - val_loss: 0.0905 - val_acc: 0.9768\n",
            "Epoch 3/4\n",
            "8000/8000 [==============================] - 150s 19ms/step - loss: 0.1180 - acc: 0.9619 - val_loss: 0.1150 - val_acc: 0.9760\n",
            "Epoch 4/4\n",
            "8000/8000 [==============================] - 146s 18ms/step - loss: 0.1062 - acc: 0.9642 - val_loss: 0.1356 - val_acc: 0.9755\n",
            "2000/2000 [==============================] - 9s 4ms/step\n",
            "Model accuracy on test set: 0.9755\n"
          ]
        }
      ],
      "source": [
        "\"\"\"It takes about 10 minutes to train on a reasonable CPU (much faster on a GPU)\n",
        "and reaches around 97% validation accuracy\"\"\"\n",
        "textmodel.fit(x_train, y_train, batch_size= 32, verbose = 1, validation_data = (x_test,y_test), epochs = 4, shuffle = True)\n",
        "loss, acc = textmodel.evaluate(x_test,y_test)\n",
        "print(\"Model accuracy on test set: {}\".format(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ehwuFm6TFVtE"
      },
      "source": [
        "## Time to test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JqyU6RVgFVtE",
        "outputId": "14b83fea-d67b-491f-9e3a-159b25927400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correctly classified ham in test set: 961, false ham: 32, true spam: 989, false spam: 18\n",
            "\n",
            "Confusion matrix:\n",
            "[[961  32]\n",
            " [ 18 989]]\n",
            "\n",
            "2000/2000 [==============================] - 9s 4ms/step\n",
            "Model accuracy on test set: 0.9755\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Check accuracy and False positives/negatives etc using confusion matrix\"\"\"\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_preds_cat = textmodel.predict(x_test)\n",
        "y_pred = [np.argmax(y_preds_cat[i]) for i in range(len(y_preds_cat))]\n",
        "\n",
        "lab = np.argmax(y_test, axis = 1)\n",
        "\n",
        "cm = confusion_matrix(lab,y_pred)\n",
        "print(\"Correctly classified ham in test set: {}, false ham: {}, true spam: {}, false spam: {}\".format(cm[0,0],cm[0,1],cm[1,1], cm[1,0]))\n",
        "print(\"\")\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)\n",
        "print('')\n",
        "\"\"\"Print model performance:\"\"\"\n",
        "loss, acc = textmodel.evaluate(x_test,y_test)\n",
        "print(\"Model accuracy on test set: {}\".format(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ya8IZYwvFVtH"
      },
      "source": [
        "## Test on text from dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PcBbuk6wFVtI",
        "outputId": "bd837db4-0b67-42e7-e6e1-d47db6dde2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 and/or 2 bedrooms to move into ASAP on Peel street, £126 a week with a £250 deposit but willing to wave the deposit if someone moves in within the next week or so. \n",
            "Beautiful flat with memory foam mattresses, double beds, TVs and surround sound in every room, parking on request\n",
            "\n",
            "Predicted label: \n",
            "ham\n",
            "\n",
            "True label:\n",
            "ham\n"
          ]
        }
      ],
      "source": [
        "'''Helper-function'''\n",
        "def spam_or_ham(lab):\n",
        "    if lab == 1:\n",
        "        print('spam')\n",
        "    elif lab == 0:\n",
        "        print('ham')\n",
        "\n",
        "'''Choose sample text from data set and output spam or ham'''\n",
        "def print_example(ind):\n",
        "    j = np.argmax(textmodel.predict_on_batch([[padded_posts[ind]]]))\n",
        "    u = label[ind]\n",
        "\n",
        "    print(text[ind])\n",
        "    print('\\nPredicted label: ')\n",
        "    spam_or_ham(j)\n",
        "    print('\\nTrue label:')\n",
        "    spam_or_ham(label[ind])\n",
        "\n",
        "\"\"\"Choose sample text index, ind> 5000 are spam posts (yes I noticed :))\"\"\"\n",
        "ind = 4999 #Change if you like to\n",
        "print_example(ind)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X8eY3ShAFVtL"
      },
      "source": [
        "## Test on custom text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ToUqjFNCFVtM",
        "outputId": "c1fa7396-7abc-4608-b740-a50f7d528e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CHEAP stuff click this LINK, no virus I promise\n",
            "\n",
            "spam\n",
            "\n",
            "Looking for a REALLY cheap apartment in Kreutzberg ASAP, plz help! :(\n",
            "\n",
            "ham\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.91062874, 0.12269474]], dtype=float32)"
            ]
          },
          "execution_count": 53,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Now for the fun part. Enter new text to test\"\"\"\n",
        "teststring = r\"\"\"Enter custom text here\"\"\"\n",
        "teststring1 = r\"\"\"CHEAP stuff click this LINK, no virus I promise\"\"\" #Spam-simulation (written by me)\n",
        "teststring2 = r\"\"\"Looking for a REALLY cheap apartment in Kreutzberg ASAP, plz help! :(\"\"\" #Ham-simulation (written by me)\n",
        "\n",
        "def test_sample_string(teststring):\n",
        "    \"\"\"Input: Sample string, works best with sublet-related posts in English.\n",
        "    Not working well with posts in German right now, but can be improved.\"\"\"   \n",
        "#   Some pre-processing:\n",
        "    seq1 = t.texts_to_sequences(teststring.split())\n",
        "    seq2 = pad_sequences(seq1, maxlen = max_length, padding = 'post').T[0]\n",
        "    seq3 = np.append(seq2,np.zeros(max_length-len(seq2)))\n",
        "    j = textmodel.predict_on_batch([[seq3]])\n",
        "#   Print out spam or ham and return binary (0 if ham, 1 if spam): \n",
        "    spam_or_ham(np.argmax(j))\n",
        "    return j\n",
        "\n",
        "#Uncomment below if you wish to enter a custom text.\n",
        "#print(teststring)\n",
        "#print(\"\")\n",
        "#test_sample_string(teststring)\n",
        "print(\"\")\n",
        "print(teststring1)\n",
        "print(\"\")\n",
        "test_sample_string(teststring1)\n",
        "print(\"\")\n",
        "print(teststring2)\n",
        "print(\"\")\n",
        "test_sample_string(teststring2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZAgXBoXHFVtP"
      },
      "source": [
        "# Unpolished ensemble model using other features (has_link and has_image)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1hSwPB6CFVtQ"
      },
      "source": [
        "I create two separate models, each for prediction using only one feature, then in the end I combine the predictions of the three models. But first, the two simple models need to be created and trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GolB5V60FVtR"
      },
      "outputs": [],
      "source": [
        "\"\"\"To further improve model predictions, you could use a weighted ensemble output from predictions of the other features.\n",
        "Tuning the weights would require a separate validation set to avoid overfitting on the dataset,\n",
        "but as the dataset is quite large I think it is possible.\"\"\"\n",
        "\n",
        "'''Create Multilayer perceptron model for image feature prediction'''\n",
        "input_links = Input(shape=(2,))\n",
        "dense_links = Dense(50, activation = 'relu')(input_links)\n",
        "dense_links2 = Dense(20, activation = 'relu')(dense_links)\n",
        "output_link = Dense(2, activation = 'sigmoid')(dense_links2)\n",
        "link_model = Model(input_links, output_link)\n",
        "link_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "'''Create Multilayer perceptron model for image feature prediction'''\n",
        "input_ims = Input(shape=(2,))\n",
        "dense_ims = Dense(50, activation = 'relu')(input_ims)\n",
        "dense_ims2 = Dense(20, activation = 'relu')(dense_ims)\n",
        "output_im = Dense(2, activation = 'sigmoid')(dense_ims2)\n",
        "im_model = Model(input_ims, output_im)\n",
        "im_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "#Shuffle data\n",
        "x_train_link = one_hot_has_link[shuffled_indices[:split]]\n",
        "x_test_link = one_hot_has_link[shuffled_indices[split:]]\n",
        "\n",
        "x_train_im = one_hot_has_image[shuffled_indices[:split]]\n",
        "x_test_im = one_hot_has_image[shuffled_indices[split:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PH8oE-vFFVtT",
        "outputId": "87894231-cf28-46e8-b275-13cdb9078875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 0s 56us/step - loss: 0.6211 - acc: 0.6472 - val_loss: 0.5929 - val_acc: 0.6565\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.6006 - acc: 0.6549 - val_loss: 0.5924 - val_acc: 0.6565\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 0s 18us/step - loss: 0.6002 - acc: 0.6549 - val_loss: 0.5930 - val_acc: 0.6565\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 0s 19us/step - loss: 0.6003 - acc: 0.6549 - val_loss: 0.5926 - val_acc: 0.6565\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.6001 - acc: 0.6549 - val_loss: 0.5928 - val_acc: 0.6565\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 0s 19us/step - loss: 0.6001 - acc: 0.6549 - val_loss: 0.5926 - val_acc: 0.6565\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.6000 - acc: 0.6549 - val_loss: 0.5923 - val_acc: 0.6565\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 0s 18us/step - loss: 0.6001 - acc: 0.6549 - val_loss: 0.5921 - val_acc: 0.6565\n",
            "Epoch 9/10\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.6002 - acc: 0.6549 - val_loss: 0.5928 - val_acc: 0.6565\n",
            "Epoch 10/10\n",
            "8000/8000 [==============================] - 0s 22us/step - loss: 0.6003 - acc: 0.6549 - val_loss: 0.5924 - val_acc: 0.6565\n",
            "2000/2000 [==============================] - 0s 3us/step\n",
            "Link model accuracy on test set: 0.6565\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 0s 53us/step - loss: 0.5670 - acc: 0.7461 - val_loss: 0.5182 - val_acc: 0.7780\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.5270 - acc: 0.7714 - val_loss: 0.5176 - val_acc: 0.7780\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 0s 19us/step - loss: 0.5269 - acc: 0.7714 - val_loss: 0.5175 - val_acc: 0.7780\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.5269 - acc: 0.7714 - val_loss: 0.5176 - val_acc: 0.7780\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.5270 - acc: 0.7714 - val_loss: 0.5176 - val_acc: 0.7780\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 0s 21us/step - loss: 0.5269 - acc: 0.7714 - val_loss: 0.5181 - val_acc: 0.7780\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.5267 - acc: 0.7714 - val_loss: 0.5187 - val_acc: 0.7780\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.5268 - acc: 0.7714 - val_loss: 0.5175 - val_acc: 0.7780\n",
            "Epoch 9/10\n",
            "8000/8000 [==============================] - 0s 19us/step - loss: 0.5262 - acc: 0.7714 - val_loss: 0.5190 - val_acc: 0.7780\n",
            "Epoch 10/10\n",
            "8000/8000 [==============================] - 0s 20us/step - loss: 0.5268 - acc: 0.7714 - val_loss: 0.5179 - val_acc: 0.7780\n",
            "2000/2000 [==============================] - 0s 8us/step\n",
            "Image model accuracy on test set: 0.778\n"
          ]
        }
      ],
      "source": [
        "link_model.fit(x_train_link, y_train, batch_size= 32, verbose = 1, validation_data = (x_test_link,y_test), epochs = 10, shuffle = True)\n",
        "loss2, acc2 = link_model.evaluate(x_test_link,y_test)\n",
        "print(\"\")\n",
        "print(\"Link model accuracy on test set: {}\".format(acc2))\n",
        "print(\"\")\n",
        "\n",
        "im_model.fit(x_train_im, y_train, batch_size= 32, verbose = 1, validation_data = (x_test_im,y_test), epochs = 10, shuffle = True)\n",
        "loss3, acc3 = im_model.evaluate(x_test_im,y_test)\n",
        "print(\"\")\n",
        "print(\"Image model accuracy on test set: {}\".format(acc3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7AC5Ki9KFVtW",
        "outputId": "6eab377c-a546-4a81-ed9b-122f1b6b3d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Seeking either a studio, one bedroom or single room in a two bedroom apartment in Back Bay, South End, North End or Cambridge. Looking to spend under $1,500/month. Can't move in until the Spring.\n",
            "\n",
            "ham\n",
            "[0.9994343  0.00222316] Text model prediction\n",
            "[0.58995295 0.40849602] Link model prediction\n",
            "[0.7118904 0.2898174] Image model prediction\n",
            "[0.7670925  0.23351221] Ensemble prediction\n",
            "[1,0] means ham and [0,1] spam\n",
            "\n",
            "True label: [1. 0.]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.7670925 , 0.23351221], dtype=float32)"
            ]
          },
          "execution_count": 35,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ensemble_prediction_on_text(index, weights = [1,1,1]):\n",
        "    \"\"\"Prints the predictions of the three different models,\n",
        "    and returns (and prints) the ensemble average prediction, given an index and weights\n",
        "    (by default [1,1,1] i.e. assuming no model is better than the others).\"\"\"\n",
        "    w1, w2, w3 = weights[0], weights[1], weights[2]\n",
        "    print(\"\")\n",
        "    print(text[index])\n",
        "    print(\"\")\n",
        "    text_pred = test_sample_string(text[index])[0]\n",
        "    print(\"%s Text model prediction\" % text_pred)\n",
        "    link_pred = link_model.predict_on_batch([[one_hot_has_link[index]]])[0]\n",
        "    print(\"%s Link model prediction\" % link_pred)\n",
        "    \n",
        "    im_pred = im_model.predict_on_batch([[one_hot_has_image[index]]])[0]\n",
        "    print(\"%s Image model prediction\" % im_pred)\n",
        "    \n",
        "    ensemble_pred = np.mean([w1*text_pred,w2*link_pred,w3*im_pred], axis = 0)\n",
        "    print(\"%s Ensemble prediction\" % ensemble_pred)\n",
        "    print(\"[1,0] means ham and [0,1] spam\")\n",
        "    print(\"\")\n",
        "    print(\"True label: {}\".format(one_hot_labels[index]))\n",
        "    return ensemble_pred\n",
        "\n",
        "#Same function, but without printing\n",
        "def ensemble_prediction_on_text_no_prints(index, weights = [1,1,1]):\n",
        "    \"\"\"Prints the predictions of the three different models,\n",
        "    and returns (and prints) the ensemble average prediction, given an index and weights\n",
        "    (by default [1,1,1] i.e. assuming no model is better than the others).\"\"\"\n",
        "    w1, w2, w3 = weights[0], weights[1], weights[2]\n",
        "    text_pred = test_sample_string(text[index])[0]\n",
        "\n",
        "    link_pred = link_model.predict_on_batch([[one_hot_has_link[index]]])[0]\n",
        "    \n",
        "    im_pred = im_model.predict_on_batch([[one_hot_has_image[index]]])[0]\n",
        "    \n",
        "    ensemble_pred = np.mean([w1*text_pred,w2*link_pred,w3*im_pred], axis = 0)\n",
        "\n",
        "    return ensemble_pred\n",
        "\n",
        "ensemble_prediction_on_text(2) #Change index if you want to try other posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "I_wTjMV2FVtZ",
        "outputId": "557fe856-57b0-407f-b41c-290a82b4fb26"
      },
      "outputs": [],
      "source": [
        "\"\"\"THIS PART IS NOT REALLY FINISHED, BUT IT MAY BE A WAY TO IMPROVE THE MODEL\"\"\"\n",
        "\"\"\"A crude estimate of performance of the ensemble model. The weights should be tuned and the model should be tested on a separate test set, not used for optimizing the weights,\n",
        " but I save that for future work.\"\"\"\n",
        "corr = 0\n",
        "count = 0\n",
        "'''Takes a little while, could certainly be optimized'''\n",
        "for index in range(10000): #Note, it is not really good practice to test it on the whole dataset, as it has been used for training, this is just to get an estimate of performance.\n",
        "    if index == 7178: #Troublesome post, ignore for now\n",
        "        continue\n",
        "    k = np.argmax(ensemble_prediction_on_text_no_prints(index, [acc, acc2, acc3])) #Weight the model predictions by their performance, to begin with.\n",
        "    if k == label[index]:\n",
        "        corr += 1\n",
        "    count += 1\n",
        "\n",
        "print(\"Accuracy of ensemble model: {}\".format(corr/count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "egyp_9JhFVtc"
      },
      "source": [
        "# FINAL THOUGHTS\n",
        "\n",
        "The text based CNN classifier obtained an accuracy of 97%, and the ensemble method roughly the same. There are many hyper-parameters to tweak, so I think you could improve performance quite a lot. The nice thing about using Deep Learning is that you don't need to manually find features in the text, the model finds the relevant features during training. If there are some features that might be relevant you can still add them to an ensemble model as described above. One useful feature would be to check a user identity against a blacklist of known facebook-spammers. They (the spammers) probably change account now and then, but it could make the model faster for a while at least."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DXewuc1HFVtd"
      },
      "source": [
        "# What would I do if there were no labels available?\n",
        "Unsupervised, the problem gets harder, of course. There are still some techniques that you can use, \n",
        "\n",
        "for example k-means clustering (with two clusters, assigned as spam or ham). Semi-supervized techniques using both unsupervised clustering of data and then classification with the available but few labels could improve performance.\n",
        "\n",
        "## Do I think it would work against scammers?\n",
        "Although unsupervised methods are not guaranteed to perform great, they will usually do better than doing nothing at all.\n",
        "The best way to find out is to build and optimize a k-means clustering algorithm and check it against labelled data to test performance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "assignment2notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
